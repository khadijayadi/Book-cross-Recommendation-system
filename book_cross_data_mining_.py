# -*- coding: utf-8 -*-
"""book-cross-data-mining .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gTnjA6bQsb1VoZV-xA0gscoxelvGl9-p

# **used libraries**
"""

import pandas as pd
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import plotly.express as px
import plotly.graph_objects as go
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
from sklearn.preprocessing import LabelEncoder
from scipy.sparse import csr_matrix
from sklearn.feature_extraction.text import TfidfVectorizer
from difflib import get_close_matches
from math import sqrt
from sklearn.metrics import mean_squared_error
import numpy as np
import random
from tqdm import tqdm

random.seed(42)
np.random.seed(42)

"""# 1.Data Understanding"""

# load the datastes

books_df = pd.read_csv("Books.csv", encoding="latin-1", low_memory=False)
ratings_df = pd.read_csv("Ratings.csv", encoding="latin-1")
users_df = pd.read_csv("Users.csv", encoding="latin-1")

print(" Books:", books_df.shape)
print(" Ratings:", ratings_df.shape)
print(" Users:", users_df.shape)

""" Used (encoding='latin-1') to handle special characters because some of the book titles or names are not in english , and (low_memory=False) to prevent dtype guessing issues in large files so that the code can read the csv file than later make modification

"""

books_df.info()
ratings_df.info()
users_df.info()

print('books:')
display(books_df.head())
print('rating:')
display(ratings_df.head())
print('users:')
display(users_df.head())

# Books
books_df['ISBN'].nunique()
books_df['Book-Title'].nunique()
books_df['Book-Author'].nunique()
books_df['Publisher'].nunique()

# Ratings
ratings_df['User-ID'].nunique()
ratings_df['ISBN'].nunique()
ratings_df['Book-Rating'].min(), ratings_df['Book-Rating'].max()

# Users
users_df['User-ID'].nunique()
users_df['Location'].nunique()
users_df['Age'].isnull().sum()

"""# 2.Data Preprocessing

# 2.1.Data Cleaning :
"""

# cleaning the books Dataset


books_df.drop(columns=['Image-URL-S', 'Image-URL-M', 'Image-URL-L'], inplace=True)


books_df.dropna(subset=['Book-Title', 'Book-Author', 'Publisher'], inplace=True)


books_df['Year-Of-Publication'] = pd.to_numeric(books_df['Year-Of-Publication'], errors='coerce')
books_df = books_df[(books_df['Year-Of-Publication'] >= 1000) & (books_df['Year-Of-Publication'] <= 2025)]

"""so here for this dataset's cleaning as we said before we are dropping the image URL columns , we are also dropping the entity row where there is a missing information about either of the Book title  or the author, as well as the book Publisher

Then one other change is modifying the type of 'the year of publication' to a numeric value as well as stating errors in that column as NaN . we also dropped the books with invalid year date .  
"""

# cleaning the ratings dataset
# Here we are only removing the ratings with value 0 because they serve as implicit feedback

ratings_df = ratings_df[ratings_df['Book-Rating'] > 0]

# cleaning the users dataset

users_df = users_df.dropna(subset=['Age'])

users_df = users_df[(users_df['Age'] >= 10) & (users_df['Age'] <= 100)]

"""We are dropping the users with missing ages and keeping only realistic age vlaues (greater than 10 and less that 100) for a clean dataset .

"""

# for more recommendation  accuracy we are  removing the very rare users or unpopular books

# Rare users are the ones with less than 5 ratings
active_users = ratings_df['User-ID'].value_counts()
ratings_df = ratings_df[ratings_df['User-ID'].isin(active_users[active_users >= 5].index)]

# Rare books are the books that have less than 10 ratings
popular_books = ratings_df['ISBN'].value_counts()
ratings_df = ratings_df[ratings_df['ISBN'].isin(popular_books[popular_books >= 10].index)]

# display the new clean dataset after merging

ratings_with_books = pd.merge(ratings_df, books_df, on='ISBN', how='inner')
full_df = pd.merge(ratings_with_books, users_df, on='User-ID', how='inner')

#show only the first 10 rows
display(full_df.head(100))

num_users = full_df['User-ID'].nunique()
num_books = full_df['ISBN'].nunique()
num_ratings = full_df.shape[0]

num_users, num_books, num_ratings

"""# 2.2.Train-Test Split:

"""

# I used the final merged dataset for splitting
# Get rid of duplicates
full_df.drop_duplicates(subset=['User-ID', 'ISBN'], inplace=True)

# 80%-20% SPLIT
train_df, test_df = train_test_split(full_df, test_size=0.2, random_state=42)


print("Training set:", train_df.shape)
print("Testing set:", test_df.shape)

"""# 3.Data Analysis -Visualisation of patterns-"""

#1. Histogram : Books ratings'distribution

# used matplotlib
plt.figure(figsize=(8, 5))
rating_counts = full_df['Book-Rating'].value_counts().sort_index()
bars = plt.bar(rating_counts.index.astype(str), rating_counts.values, color='skyblue', edgecolor='black')
plt.title('Distribution of Book Ratings')
plt.xlabel('Rating')
plt.ylabel('Number of Ratings')
plt.grid(axis='y')

# in top of each bar will show the count of that rating score
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2.0, yval + 100, int(yval), ha='center', va='bottom')
plt.grid(False)
plt.tight_layout()
plt.show()

"""The histogram shows rating distribution and it reaveals a right-skeweed histogram which means positive bias toward high ratings.

"""

# ratings check : I will check if the low ratings are valuable for giving me insights for the next visualisatiosn
# so here we are checking the percentage of high and low ratings into the entire clean dataset

rating_bins = ['1-4 (Low Ratings)', '5-10 (High Ratings)']
rating_categories = full_df['Book-Rating'].apply(lambda x: '1-4 (Low Ratings)' if 1 <= x <= 4 else '5-10 (High Ratings)')


rating_distribution = rating_categories.value_counts().reindex(rating_bins)


plt.figure(figsize=(6, 6))
plt.pie(rating_distribution, labels=rating_bins, autopct='%1.1f%%', colors=['salmon', 'lightgreen'])
plt.title('Proportion of Low vs High Ratings')
plt.tight_layout()
plt.show()

"""As seen in this pie chart the high ratings which means from 5 to 10 are 96.6% which means that are dominant and that's why i will continue with considering only this rating score so 5 will be the lowest rating and 10 is the highest"""

# 2. Line Chart: (User Age vs Rating Behavior)
#used Plotly express

avg_rating_by_age = full_df.groupby('Age')['Book-Rating'].mean().reset_index()
fig = px.line(
    avg_rating_by_age,
    x='Age',
    y='Book-Rating',
    title='Average Book Rating by User Age',
    markers=True,
    template='plotly_white'
)
fig.update_traces(line=dict(color='darkblue'))
fig.update_layout(title={'x': 0.5}, xaxis_tickangle=-45)
fig.show()

"""The line chart uncovers slight differences in rating tendencies by age group which shows that users under 20 assumed to be teens are most likely to give a good rating while users aged between 80 and above they are giving more negative reviews

"""

# 3. Bar Chart: user activity
# used plotly express

top_users = full_df['User-ID'].value_counts().head(10).reset_index()
top_users.columns = ['User-ID', 'Rating Count']


fig = px.bar(
    top_users,
    x='User-ID',
    y='Rating Count',
    title='Top 10 Most Active Users by Rating Count',
    labels={'User-ID': 'User ID', 'Rating Count': 'Number of Ratings'},
    template='plotly_white',
    text='Rating Count'
)

# Adjust title and layout
fig.update_traces(marker_color='lightskyblue', textposition='outside')
fig.update_layout(title={'x': 0.5}, xaxis_tickangle=-45)

fig.show()

"""The bar chart of top active users shows who the most important users are, which is useful for collaborative filtering.

# 4. Data modeling

# 4.1 Collaborative filtering
"""

user_item_matrix = full_df.pivot_table(index='User-ID', columns='Book-Title', values='Book-Rating')
user_item_matrix_filled = user_item_matrix.fillna(0)

user_item_matrix_filled.shape
user_item_matrix_filled.head()

"""#Switching to Sparseâ€‚Matrix for Collaborative Filtering

Using ''full_df.pivot_table'', we initially attempted to create an entire user-item matrix and fill in any values with zeros. But due to its compact matrix representation, this approach led to memory issues, mainly due to the high dimensionality of the merged dataset

To overcomeâ€‚this, we used sparse matrix format through ''scipy.sparse'', This technique directly occupies memory for the non-empty values (actual ratings) and allows space to be used without loss for the empty ones. It's great for large-scale collaborative filtering and it means we can continue through similarity calculations without being burdened by performance.
"""

#Item-based CF

# Encode User-ID and Book-Title to numeric indices
user_encoder = LabelEncoder()
book_encoder = LabelEncoder()

full_df['user_idx'] = user_encoder.fit_transform(full_df['User-ID'])
full_df['book_idx'] = book_encoder.fit_transform(full_df['Book-Title'])

# Create a user-item sparse matrix: rows = users, columns = books, values = ratings
sparse_matrix = csr_matrix(
    (full_df['Book-Rating'], (full_df['user_idx'], full_df['book_idx']))
)

sparse_matrix.shape

# This gives us item-item COSINE similarity matrix (book-based (book X book) collaborative filtering)
item_similarity = cosine_similarity(sparse_matrix.T) # here also transposed the matrix

# Store results in a DataFrame
item_similarity_df = pd.DataFrame(item_similarity)
item_similarity_df.shape
display(item_similarity_df.head(100))

display(sparse_matrix)

"""We calculated one item-to-item similarityâ€‚matrix by using cosine similarity on this user-book matrix. Each value indicates how similar two books are compared to their pattern of user ratings.

This complete similarity matrix (book Ã— book) is the basis of our item-based collaborative filtering model. Itâ€‚enables us to suggest books, which share rating pattern with books that user has already read or intereacted with.
"""

# Function to get top 5 similar books given a book index

def get_index_by_title(title):
    matches = full_df[full_df['Book-Title'].str.lower().str.strip() == title.lower().strip()]
    if not matches.empty:
        return matches.index[0]
    else:
        return None

# Find book index using LabelEncoder in the previous cell
def recommend_books_by_title(title, top_n=5):
    try:
        book_idx = book_encoder.transform([title])[0]
    except ValueError:
        print(f" Book titled '{title}' not found in dataset.\n")
        print(" Suggested similar titles:")
        suggestions = [t for t in full_df['Book-Title'].unique() if title.lower() in t.lower()]
        for s in suggestions[:5]:
            print(f" - {s}")
        return

    # print the similarity of the suggested books to make more clear why we choosed them
    similarity_scores = list(enumerate(item_similarity[book_idx]))
    similarity_scores = sorted(similarity_scores, key=lambda x: x[1], reverse=True)

    print(f"\n Books similar to '{title}':\n")

    count = 0
    for idx, score in similarity_scores[1:]:  # skip the book itself
        similar_title = book_encoder.inverse_transform([idx])[0]#return the title no the numerical value that we already encocoded before
        print(f"{count+1}. {similar_title} | Similarity: {score:.4f}")
        count += 1
        if count == top_n:
            break

# View some book titles to check formatting
full_df['Book-Title'].drop_duplicates().sort_values().head(10).tolist()

# Example usage
example_title = "311 Pelican Court"
recommend_books_by_title(example_title, top_n=5)

"""# How Item-Based Collaborative Filtering isâ€‚implemented

The Item-based collaborative filtering model was implemented byâ€‚observing User rating  in all books. First, I created a sparse matrixâ€‚where rows were users, columns were books, and each cell was a rating of a user.

I can now compare cosine similarityâ€‚for every book to every other book, based on how users rated  both pairs. The resultâ€‚is a similarity matrix where each value indicates how similar two books are to each other

 Theâ€‚recommendation system produce its predictions as follows:

1.Take a given book title,

2.Findâ€‚its index in the similarity matrix.

3.Get the top 5 similar books (excluding itself) from theâ€‚shortlist,

4.return the most 5 suggested books.

This model positsâ€‚that if two books have consistently received similar ratings from lots of people, they treat shared themes/genre/quality, and thus are good recommendations for one another.
"""

#User-based CF

# to make sure the model works and don't crash due to the huge amount of data
# I dropped all the null values , and reformatted the  sparse matrix

full_df = full_df.dropna(subset=['Book-Rating', 'User-ID', 'Book-Title'])

# Rebuild user-item sparse matrix
user_item_matrix = csr_matrix(
    (full_df['Book-Rating'], (full_df['user_idx'], full_df['book_idx']))
)

# compute the cosine similarity for the user-item matrix
user_similarity = cosine_similarity(user_item_matrix)
user_similarity_df = pd.DataFrame(user_similarity)# now we have a user-to-user similarity matrix

user_similarity_df.shape
display(user_similarity_df.head(100))

"""For user-based collaborative filtering, we constructed a user-item matrix from the processed vectors and then calculated cosineâ€‚similarity between users. Memory resource constraints madeâ€‚the model run on a 8,212 users filtered subset which we cleaned before running the cosine similarity . With this similarity matrix, the model will be ableâ€‚to recommend books using similitude between users with similar rating behavior. This alghorithm will help reveal such communities of users with common interests and recommend books that these users have not rated yet but we assume that they will like based on their previous behaviour ."""

# Function to get the top 5 recommended books (unread by the user, given his index ) based on similar users

def recommend_books_by_user(user_idx, top_n=100):
    similarity_scores = list(enumerate(user_similarity[user_idx]))
    similarity_scores = sorted(similarity_scores, key=lambda x: x[1], reverse=True)
    top_users = [idx for idx, score in similarity_scores[1:11]]

    rated_books = full_df[full_df['user_idx'] == user_idx]['book_idx'].tolist()
    candidate_books = full_df[
        (full_df['user_idx'].isin(top_users)) &
        (~full_df['book_idx'].isin(rated_books))
    ]

    top_books = candidate_books.groupby('Book-Title')['Book-Rating'].mean().sort_values(ascending=False).head(top_n)

    print(f"\n Recommended books for user {user_idx}:\n")
    for i, (title, score) in enumerate(top_books.items(), 1):
        print(f"{i}. {title} ")

# Example usage
recommend_books_by_user(user_idx=0, top_n=5)

"""# How User-Item Collaborative filtering is implemented

To conduct user-based CF, a user-item rating matrix was created based on theâ€‚preprocessed and filtered dataset, where every row represents a user and every column represents a book. Cosine similarity was then used on this matrix to calculate how similar a user isâ€‚to each other user based on their rating preferences.

For each target user, the system selects the 100 most similar users and recommends the 5 most top rated books by these similar users that have not already been rated by the target user.

 This approach endorses that users sharingâ€‚similar behavioural patterns earlier are likely to enjoy similar content in the future.

# 4.2 Content-Based filtering
"""

# Filter to get books that are  actually rated in the full merged dataset
rated_books_df = pd.merge(ratings_df, books_df, on="ISBN", how="inner")
rated_books_df = rated_books_df.drop_duplicates(subset=["Book-Title", "Book-Author"])
rated_books_df.reset_index(drop=True, inplace=True)

# Combine features
rated_books_df['combined_features'] = rated_books_df['Book-Title'] + ' ' + rated_books_df['Book-Author']

# TF-IDF Vectorizer
tfidf = TfidfVectorizer(stop_words='english')
tfidf_matrix = tfidf.fit_transform(rated_books_df['combined_features'])

# Cosine similarity
content_similarity = cosine_similarity(tfidf_matrix)
content_similarity_df = pd.DataFrame(content_similarity)
content_similarity_df.shape
display(content_similarity_df.head(100))

"""
We built our Content-Based Recommendation model by using a TF-IDF vectorizer on the combined 'Book Title' and 'Book Author' fields. This allowed us to numerically represent each book based on its metadata. We then calculated cosine similarity between all books, giving us a matrix of how similar each book is to others in terms of content. This matrix will now allow us to recommend books with similar content
"""

#  content-based recommendation function with fuzzy matching fallback

title_to_index = pd.Series(rated_books_df.index, index=rated_books_df['Book-Title'].str.lower())


def recommend_books_content_based(title, top_n=5):
    title = title.lower().strip()
    # Attempt to find an exact match
    if title not in title_to_index:
        print(f" Book titled '{title}' not found in the dataset.\n")

        # Fuzzy match suggestions
        all_titles = rated_books_df['Book-Title'].str.lower().unique()
        suggestions = get_close_matches(title, all_titles, n=5, cutoff=0.5)

        if suggestions:
            print("ðŸ” Did you mean one of the following?")
            for s in suggestions:
                print(f" - {s}")
        else:
            print(" No similar titles found.")
        return

    # Get index of the book
    idx = title_to_index[title]

    # Compute similarity scores
    similarity_scores = list(enumerate(content_similarity[idx]))
    similarity_scores = sorted(similarity_scores, key=lambda x: x[1], reverse=True)

    # Display top 5 similar books
    print(f"\n Books similar to '{rated_books_df.iloc[idx]['Book-Title']}':\n")
    count = 0
    for i, score in similarity_scores[1:]:
        similar_title = rated_books_df.iloc[i]['Book-Title']
        similar_author = rated_books_df.iloc[i]['Book-Author']
        print(f"{count+1}. {similar_title} by {similar_author} | Similarity: {score:.4f}")
        count += 1
        if count == top_n:
            break

recommend_books_content_based("A Place Called Freedom", top_n=5)

"""# 5. Evaluation using MSE"""

# prepare the dataframes and matrices are ready for evaluation

book_encoder = LabelEncoder()

train_df['user_idx'] = user_encoder.fit_transform(train_df['User-ID'])
train_df['book_idx'] = book_encoder.fit_transform(train_df['Book-Title'])

# Ensure test set contains only books and users in training
test_df = test_df[test_df['User-ID'].isin(user_encoder.classes_)]
test_df = test_df[test_df['Book-Title'].isin(book_encoder.classes_)]
test_df['user_idx'] = user_encoder.transform(test_df['User-ID'])
test_df['book_idx'] = book_encoder.transform(test_df['Book-Title'])

#Create user-item sparse matrix from training data
sparse_train = csr_matrix((train_df['Book-Rating'], (train_df['user_idx'], train_df['book_idx'])))

#Compute item-to-item similarity
item_similarity = cosine_similarity(sparse_train.T)

"""# 5.1 IBCF evaluation :"""

# define the predection function for item-based CF
def predict_item_cf(user_idx, book_idx, k=5):
    user_ratings = sparse_train[user_idx, :].toarray().flatten()
    sim_scores = item_similarity[book_idx]
    rated_indices = user_ratings.nonzero()[0]

    if len(rated_indices) == 0:
        return 5.0  # Default average rating

    similarities = sim_scores[rated_indices]
    ratings = user_ratings[rated_indices]

    if similarities.sum() == 0:
        return ratings.mean()

    weighted_sum = (similarities * ratings).sum()
    return weighted_sum / similarities.sum()

# Evaluate predictions on updated test set
item_cf_predictions = []
item_cf_truths = []

for _, row in test_df.iterrows():
    pred = predict_item_cf(row['user_idx'], row['book_idx'])
    item_cf_predictions.append(pred)
    item_cf_truths.append(row['Book-Rating'])

# Calculate RMSE
item_cf_rmse = sqrt(mean_squared_error(item_cf_truths, item_cf_predictions))
print("Item-Based CF RMSE:", item_cf_rmse)

"""#5.2 UBCF evaluation :"""

# define the predection function for UBCF
def predict_user_cf(user_idx, book_idx, k=5):
    # Users who rated the book
    book_ratings = sparse_train[:, book_idx].toarray().flatten()
    rated_by_users = book_ratings.nonzero()[0]

    if len(rated_by_users) == 0:
        return 5.0

    sim_scores = user_similarity[user_idx][rated_by_users]
    ratings = book_ratings[rated_by_users]

    if sim_scores.sum() == 0:
        return ratings.mean()

    weighted_sum = (sim_scores * ratings).sum()
    return weighted_sum / sim_scores.sum()

# Make predictions on the test set
user_cf_predictions = []
user_cf_truths = []

for _, row in test_df.iterrows():
    pred = predict_user_cf(row['user_idx'], row['book_idx'])
    user_cf_predictions.append(pred)
    user_cf_truths.append(row['Book-Rating'])

#Compute RMSE
user_cf_rmse = sqrt(mean_squared_error(user_cf_truths, user_cf_predictions))


print("User-Based CF RMSE:", user_cf_rmse)

"""# 5.3 CBF Evaluation :"""

# Check the test dataframe
if 'Book-Author' in test_df.columns:
    test_df.drop(columns=['Book-Author'], inplace=True)

test_df = pd.merge(test_df, books_df[['Book-Title', 'Book-Author']], on='Book-Title', how='left')
rated_books_df = rated_books_df.sort_values(by='Book-Title')

# Mapping title to index
title_to_index = pd.Series(rated_books_df.index, index=rated_books_df['Book-Title'])

#Define prediction function (for a given title)
def predict_cb_rating(title, top_n=5):
    if title not in title_to_index:
        return 5.0

    idx = title_to_index[title]
    if isinstance(idx, pd.Series):
        idx = idx.iloc[0]

    sim_scores = list(enumerate(content_similarity[idx]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:top_n+1]
    book_indices = [i[0] for i in sim_scores]

    return rated_books_df.iloc[book_indices]['Book-Rating'].mean()

# compute predictions for each unique title
unique_titles = test_df['Book-Title'].unique()
title_predictions = {}

for title in tqdm(unique_titles, desc="Computing predictions"):
    title_predictions[title] = predict_cb_rating(title)

# Map predictions to the test set
test_df['predicted_cb_rating'] = test_df['Book-Title'].map(title_predictions)

# Compute RMSE
cb_rmse = sqrt(mean_squared_error(test_df['Book-Rating'], test_df['predicted_cb_rating']))
print("Content-Based CF RMSE:", cb_rmse)

import matplotlib.pyplot as plt

# RMSE values
item_cf_rmse = item_cf_rmse
user_cf_rmse = user_cf_rmse
content_rmse = cb_rmse

# Prepare data for plotting
models = ['Item-Based CF', 'User-Based CF', 'Content-Based']
rmse_scores = [item_cf_rmse, user_cf_rmse, content_rmse]


plt.figure(figsize=(8, 5))
bars = plt.bar(models, rmse_scores, color=['steelblue', 'indianred', 'seagreen'])
plt.ylabel("RMSE")
plt.title("Comparison of Recommendation Models", fontsize=14, fontweight='bold')
plt.ylim(0, max(rmse_scores) + 1)

# add labels
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.05, round(yval, 3), ha='center', va='bottom')

plt.tight_layout()
plt.show()